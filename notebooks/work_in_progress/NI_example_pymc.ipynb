{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes PRF using pymc\n",
    "- not implemented properly yet...\n",
    "Load eg data + design matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './design_matrix.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mprfpy_bayes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# If you already have the design matrix as a numpy array, you can simply load it...\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m dm \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./design_matrix.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# You can see we have a binarized matrix, of a bar moving across the screen...\u001b[39;00m\n\u001b[1;32m     18\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure()\n",
      "File \u001b[0;32m~/.conda/envs/prf_fix/lib/python3.13/site-packages/numpy/lib/_npyio_impl.py:459\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    457\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    460\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './design_matrix.npy'"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    from prfpy_csenf.stimulus import PRFStimulus2D\n",
    "    from prfpy_csenf.model import Iso2DGaussianModel, Norm_Iso2DGaussianModel\n",
    "except:\n",
    "    from prfpy.stimulus import PRFStimulus2D\n",
    "    from prfpy.model import Iso2DGaussianModel, Norm_Iso2DGaussianModel\n",
    "from datetime import datetime\n",
    "from prfpy_bayes.utils import *\n",
    "\n",
    "# If you already have the design matrix as a numpy array, you can simply load it...\n",
    "dm = np.load('./design_matrix.npy')\n",
    "\n",
    "# You can see we have a binarized matrix, of a bar moving across the screen...\n",
    "fig = plt.figure()\n",
    "rows = 10\n",
    "cols = 10\n",
    "fig.set_size_inches(5,5)\n",
    "for i in range(100):\n",
    "    ax = fig.add_subplot(rows, cols, i+1)\n",
    "    ax.imshow(dm[:,:,i], vmin=0, vmax=1)\n",
    "    ax.axis('off')\n",
    "# Now we need to enter the design matrix in a way that prfpy can read it. \n",
    "prf_stim = PRFStimulus2D(\n",
    "    screen_size_cm=39.3,          # Distance of screen to eye\n",
    "    screen_distance_cm=196,  # height of the screen (i.e., the diameter of the stimulated region)\n",
    "    design_matrix=dm,                                   # dm (npix x npix x time_points)\n",
    "    TR=1.5,                                  # TR\n",
    "    )\n",
    "# Now we can make the model\n",
    "g_model = Iso2DGaussianModel(stimulus=prf_stim, hrf=[1, 4.6, 0], normalize_RFs=True)\n",
    "#dn_model = Norm_Iso2DGaussianModel(stimulus=prf_stim, hrf=[1, 4.6, 0])\n",
    "\n",
    "# Data\n",
    "psc_runs = np.load('eg_data.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PYMC as a black box...\n",
    "Why do this? Not necessarily the most efficient; but it means you don't have to redefine everything\n",
    "\n",
    "https://www.pymc.io/projects/examples/en/latest/howto/blackbox_external_likelihood_numpy.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytensor\n",
    "\n",
    "import pytensor.tensor as pt\n",
    "\n",
    "from pytensor.graph import Apply, Op\n",
    "from scipy.optimize import approx_fprime\n",
    "import scipy.stats\n",
    "cdf = scipy.stats.norm.cdf\n",
    "\n",
    "pytensor.config.optimizer = 'None'\n",
    "# pytensor.config.exception_verbosity = 'high'\n",
    "pytensor.config.linker = 'py'\n",
    "def my_model(l_ecc, l_pol, l_size, l_beta):\n",
    "    \"\"\"\"\"\"\n",
    "    radius = 5\n",
    "    min_size = 0.5\n",
    "\n",
    "    ecc = radius * cdf(l_ecc, 0, 1)\n",
    "    pol = 2*np.pi*cdf(l_pol, 0, 1) - np.pi\n",
    "    x = ecc*np.cos(pol)\n",
    "    y = ecc*np.sin(pol)\n",
    "    size = min_size + (radius-min_size)*cdf(l_size)\n",
    "    beta = np.exp(l_beta)\n",
    "    return g_model.return_prediction(\n",
    "        mu_x = x,\n",
    "        mu_y = y,\n",
    "        size = size,\n",
    "        beta = beta,\n",
    "        baseline = np.array([0])\n",
    "    ).squeeze()\n",
    "\n",
    "def my_loglike(l_ecc, l_pol, l_size, l_beta, data):\n",
    "    \"\"\"\"\"\"\n",
    "    radius = 5\n",
    "    min_size = 0.5\n",
    "    noise_sd = 1\n",
    "    ecc = radius * cdf(l_ecc, 0, 1)\n",
    "    pol = 2*np.pi*cdf(l_pol, 0, 1) - np.pi\n",
    "    x = ecc*np.cos(pol)\n",
    "    y = ecc*np.sin(pol)\n",
    "    size = min_size + (radius-min_size)*cdf(l_size)\n",
    "    beta = np.exp(l_beta)\n",
    "    pred = g_model.return_prediction(\n",
    "        mu_x = x,\n",
    "        mu_y = y,\n",
    "        size = size,\n",
    "        beta = beta,\n",
    "        baseline = np.array([0])\n",
    "    ).squeeze()\n",
    "    residuals = data - pred\n",
    "    log_like = -0.5 * (np.log(2*np.pi*noise_sd**2) + (residuals/noise_sd)**2)\n",
    "    \n",
    "    return log_like.astype(np.float32)\n",
    "\n",
    "class LogLike(Op):\n",
    "    def make_node(self, l_ecc, l_pol, l_size, l_beta, data\n",
    "                  ) -> Apply:\n",
    "        l_ecc = pt.as_tensor(l_ecc)\n",
    "        l_pol = pt.as_tensor(l_pol)\n",
    "        l_size = pt.as_tensor(l_size)\n",
    "        l_beta = pt.as_tensor(l_beta)\n",
    "        data = pt.as_tensor(data)\n",
    "\n",
    "        inputs = [l_ecc, l_pol, l_size, l_beta, data]\n",
    "        # Define output type, in our case a vector of likelihoods\n",
    "        # with the same dimensions and same data type as data\n",
    "        # If data must always be a vector, we could have hard-coded\n",
    "        # outputs = [pt.vector()]\n",
    "        outputs = [data.type()]\n",
    "\n",
    "        # Apply is an object that combines inputs, outputs and an Op (self)\n",
    "        return Apply(self, inputs, outputs)\n",
    "\n",
    "    def perform(self, node: Apply, inputs: list[np.ndarray], outputs: list[list[None]]) -> None:\n",
    "        # This is the method that compute numerical output\n",
    "        # given numerical inputs. Everything here is numpy arrays\n",
    "        l_ecc, l_pol, l_size, l_beta, data = inputs\n",
    "\n",
    "        # call our numpy log-likelihood function\n",
    "        loglike_eval = my_loglike(l_ecc, l_pol, l_size, l_beta, data)\n",
    "        # Save the result in the outputs list provided by PyTensor\n",
    "        # There is one list per output, each containing another list\n",
    "        # pre-populated with a `None` where the result should be saved.\n",
    "        outputs[0][0] = loglike_eval #np.asarray(loglike_eval)\n",
    "\n",
    "# Create a Op object\n",
    "loglike_op = LogLike()\n",
    "test_out = loglike_op(\n",
    "    np.array([0]),\n",
    "    np.array([0]),\n",
    "    np.array([0]),\n",
    "    np.array([0]), \n",
    "    psc_runs[0,:])\n",
    "\n",
    "pytensor.dprint(test_out)\n",
    "# os.environ['']\n",
    "test_out.eval()\n",
    "my_loglike(0, 0, 0, 0, psc_runs[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pymc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcustom_dist_loglike\u001b[39m(data, l_ecc, l_pol, l_size, l_beta):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# data, or observed is always passed as the first input of CustomDist\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loglike_op(l_ecc, l_pol, l_size, l_beta, data)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpymc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpm\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# use PyMC to sampler from log-likelihood\u001b[39;00m\n\u001b[1;32m      7\u001b[0m data \u001b[38;5;241m=\u001b[39m psc_runs[\u001b[38;5;241m0\u001b[39m,:]\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pymc'"
     ]
    }
   ],
   "source": [
    "def custom_dist_loglike(data, l_ecc, l_pol, l_size, l_beta):\n",
    "    # data, or observed is always passed as the first input of CustomDist\n",
    "    return loglike_op(l_ecc, l_pol, l_size, l_beta, data)\n",
    "\n",
    "import pymc as pm\n",
    "# use PyMC to sampler from log-likelihood\n",
    "data = psc_runs[0,:].squeeze().copy()\n",
    "with pm.Model() as no_grad_model:\n",
    "    # Priors\n",
    "    l_ecc = pm.Normal(\"l_ecc\", mu=0, sigma=1, initval=0)\n",
    "    l_pol = pm.Normal(\"l_pol\", mu=0, sigma=1, initval=0)\n",
    "    l_size = pm.Normal(\"l_size\", mu=0, sigma=1, initval=.1)\n",
    "    l_beta = pm.Normal(\"l_beta\", mu=-2, sigma=5, initval=-1)    \n",
    "\n",
    "    # use a CustomDist with a custom logp function\n",
    "    likelihood = pm.CustomDist(\n",
    "        \"likelihood\", \n",
    "        l_ecc, l_pol, l_size, l_beta,\n",
    "        observed=data,\n",
    "        logp=custom_dist_loglike,\n",
    "    )\n",
    "    ip = no_grad_model.initial_point()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "radius = 5\n",
    "min_size = .1\n",
    "\n",
    "observed_data = psc_runs[0,:].squeeze()\n",
    "with pm.Model() as model:\n",
    "    # Priors on latent parameters\n",
    "    l_ecc = pm.Normal('l_ecc', mu=0, sigma=1)\n",
    "    l_pol = pm.Normal('l_pol', mu=0, sigma=1)\n",
    "    l_size = pm.Normal('l_size', mu=0, sigma=1)\n",
    "    l_beta = pm.Normal('l_beta', mu=-2, sigma=5)\n",
    "    noise_sd = pm.HalfNormal('noise_sd', sigma=1)\n",
    "\n",
    "    logl = LogLikeGauss(\n",
    "        data=psc_runs[0,:].squeeze(),        \n",
    "    )\n",
    "\n",
    "    # conver to a tensor vector\n",
    "    theta = pt.as_tensor_variable([l_ecc, l_pol, l_size, l_beta, noise_sd])\n",
    "\n",
    "    # Use potential to \"call\" the Op \n",
    "    # and include it in the logp computation\n",
    "    pm.Potential('likelihood', logl(theta))\n",
    "\n",
    "    # Use custom number of draws to replace the HMC default\n",
    "    idata_mh = pm.sample(100, tune=10, step=pm.Slice())\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model.return_prediction(\n",
    "            mu_x = 0,\n",
    "            mu_y = 0,\n",
    "            size = 3,\n",
    "            \n",
    "            beta = 1,\n",
    "            baseline = 0,   \n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.math.er"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_model.return_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prfpy_bayes.prf_bayes import *\n",
    "# Start by making the bprf objects\n",
    "Gbprf = BayesPRF(\n",
    "    prf_params=None, # We could put in our best guess here, but we will let the model figure it out...\n",
    "    model = 'gauss',\n",
    "    prfpy_model=g_model,\n",
    "    real_ts=psc_runs,\n",
    "    beta_method='glm', # We are going to use the glm method to estimate the betas (not inside the MCMC)\n",
    "    fixed_baseline=0, # We are not going to fit the baseline\n",
    "    )\n",
    "bounds = {\n",
    "    'x': [-5, 5],\n",
    "    'y': [-5, 5],\n",
    "    'size_1': [0.1, 10],\n",
    "    'amp_1': [1, 1],\n",
    "    'bold_baseline': [0, 0],\n",
    "    'hrf_deriv' : [0, 10],\n",
    "    'hrf_disp' : [0, 10],\n",
    "}\n",
    "# Uniform prior, based on bounds \n",
    "Gbprf.add_priors_from_bounds(bounds)\n",
    "# Get everything ready...\n",
    "Gbprf.prep_info()\n",
    "\n",
    "# Let start with a single voxel\n",
    "initial_guess = [\n",
    "    0, # x\n",
    "    0, # y\n",
    "    1, # size\n",
    "    # 1, # amplitude\n",
    "    4.6, # hrf_disp\n",
    "    0, # hrf_deriv\n",
    "]\n",
    "# which timeseries to fit\n",
    "idx = 0\n",
    "\n",
    "# Use the multiprocessing pool -> to speed things up\n",
    "import multiprocessing\n",
    "# Quick check, how many cpus do we have?\n",
    "print(f\"Number of cpus: {multiprocessing.cpu_count()}\")\n",
    "n_cpus = 14 # I'm using 14 for speeeed!\n",
    "\n",
    "idx = 0 # Index of the vertex times series to fit\n",
    "n_steps = 50\n",
    "n_walkers = 20\n",
    "time_start = datetime.now()\n",
    "with multiprocessing.Pool(n_cpus) as pool:\n",
    "    # We need to set the model for the global model object\n",
    "    # this is because parallel processing does not allow for the model to be pickled\n",
    "    # If it doesn't work the first time, try again... \n",
    "    # for some reason this is not always working the first time...\n",
    "\n",
    "    prfpy_global_model.set_model(g_model)\n",
    "    Gbprf.run_mcmc_fit(\n",
    "        initial_guess=initial_guess,\n",
    "        idx=idx, \n",
    "        n_walkers=20,\n",
    "        n_steps=4000, \n",
    "        pool=pool, \n",
    "        eps=.1,\n",
    "        # What to save? \n",
    "        # we fit alot of stuff...\n",
    "        # burn_in=500,        # Remove the first n steps as burn in \n",
    "        save_top_kpsc=None,   # Save the top k% of fits? [None for all] \n",
    "        save_min_rsq=None,     # Minimum rsq to save?  [None for all]\n",
    "        save_mode='obj',    # Save mode (obj or minimal). Obj saves the whole object, minimal just the parameters        \n",
    "\n",
    "\n",
    "    )\n",
    "time_end = datetime.now()\n",
    "print(f\"Time taken: {time_end - time_start}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets plot our samples around the visual field \n",
    "Gbprf.sampler[idx].visual_field(\n",
    "    dot_col='rsq',      # Color by r squared\n",
    "    th={'min-step_id':250},              # No threshold\n",
    "    do_colbar=True,\n",
    "    dot_vmin=0,    dot_vmax=1,   \n",
    "    dot_size=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Gbprf.sampler[idx].pd_params.rsq.argmax())\n",
    "Gbprf.sampler[idx].prf_ts_plot(71532)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can we use this to look at some of the correlation structure in the parameters?\n",
    "# -> i.e. which parameters can trade off against each other?\n",
    "burn_in = 500\n",
    "\n",
    "# For example size and amplitude\n",
    "Gbprf.sampler[idx].scatter(\n",
    "    px='size_1', py='amp_1',pc='rsq',\n",
    "    vmin=0,vmax=1, \n",
    "    th={\n",
    "        'min-step_id' : burn_in, # Only include samples after burn in time point. \n",
    "        'min-rsq' : 0, # Only rsq > 0\n",
    "    },\n",
    "    do_colbar=True, \n",
    "    do_corr=True, \n",
    ")\n",
    "\n",
    "# What about size and hrf_deriv?\n",
    "plt.figure()\n",
    "Gbprf.sampler[idx].scatter(\n",
    "    px='size_1', py='hrf_deriv',pc='rsq',\n",
    "    vmin=0,vmax=1, \n",
    "    th={\n",
    "        'min-step_id' : burn_in, # Only include samples after burn in time point. \n",
    "        'min-rsq' : 0, # Only rsq > 0\n",
    "    },\n",
    "    do_colbar=True, \n",
    "    do_corr=True, \n",
    ")\n",
    "\n",
    "# What about size and hrf_disp?\n",
    "plt.figure()\n",
    "Gbprf.sampler[idx].scatter(\n",
    "    px='hrf_disp', py='hrf_deriv',pc='rsq',\n",
    "    vmin=0,vmax=1, \n",
    "    th={\n",
    "        'min-step_id' : burn_in, # Only include samples after burn in time point. \n",
    "        'min-rsq' : 0, # Only rsq > 0\n",
    "    },\n",
    "    do_colbar=True, \n",
    "    do_corr=True, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ok that was interesting. \n",
    "# Lets look at some time series! Ideally where parameters differ, so we can see this tradeoff type thing\n",
    "good_fits = np.where(Gbprf.sampler[idx].pd_params['rsq'] > 0.3)[0]\n",
    "plot_eg = 0\n",
    "size_previous = -100\n",
    "for i in good_fits:\n",
    "    size_current = Gbprf.sampler[idx].pd_params['size_1'][i]\n",
    "    if np.abs(size_current - size_previous) < 0.2:\n",
    "        continue\n",
    "    else:\n",
    "        # Plot it if the size is different enough!!\n",
    "        size_previous = size_current\n",
    "        _ = Gbprf.sampler[idx].prf_ts_plot(i)\n",
    "        plot_eg += 1\n",
    "    if plot_eg > 5:\n",
    "        break\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DN - same again but with the divisive normalisation model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake some data\n",
    "fake_params = np.array([ -1, -1,  1.36,  4.8, 0,  .4,  3.42,  0.2,\n",
    "        3.16,  4.6,  0])\n",
    "fake_ts = dn_model.return_prediction(\n",
    "    *list(fake_params),\n",
    "    )\n",
    "plt.plot(fake_ts.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfa_scripts.load_saved_info import *\n",
    "from dag_prf_utils.prfpy_ts_plotter import TSPlotter\n",
    "\n",
    "psc = load_data_tc('sub-01', 'AS0', )['AS0']\n",
    "prfpy_stim = get_prfpy_stim('sub-01', 'AS0')['AS0']\n",
    "dn_model = Norm_Iso2DGaussianModel(stimulus=prfpy_stim, hrf=[1, 4.6, 0])\n",
    "prf = TSPlotter(\n",
    "    load_data_prf('sub-01', 'AS0', 'norm')['AS0']['norm'],\n",
    "    model='norm',\n",
    "    real_ts=psc,\n",
    "    prfpy_model=dn_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vx_mask = prf.return_vx_mask({\n",
    "    'min-rsq':.1, \n",
    "    'min-size_1':0.1,\n",
    "    'min-b_val' : 200, \n",
    "}    \n",
    ")\n",
    "print(np.where(vx_mask)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prf.prf_ts_plot(542745)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prfpy_bayes.prf_bayes import BayesPRF\n",
    "# Start by making the bprf objects\n",
    "Nbprf = BayesPRF(\n",
    "    prf_params=prf.prf_params_np, # We could put in our best guess here, but we will let the model figure it out...\n",
    "    model = 'norm',\n",
    "    prfpy_model=dn_model,\n",
    "    real_ts=psc, #psc_runs,\n",
    "    beta_method='glm',\n",
    "    fixed_baseline=0, \n",
    "    )\n",
    "Nbounds = {\n",
    "    'x': [-5, 5],\n",
    "    'y': [-5, 5],\n",
    "    'size_1': [0.1, 10],\n",
    "    'amp_1': [0, 10],\n",
    "    'size_2' : [0.1, 10],\n",
    "    'amp_2' : [0, 10],\n",
    "    'b_val' : [0, 10],\n",
    "    'd_val' : [0, 10],\n",
    "    'bold_baseline': [0, 0],\n",
    "    'hrf_deriv' : [1, 1], # Not going to fit the HRF, we have enough to deal with...\n",
    "    'hrf_disp' : [0, 0],\n",
    "}\n",
    "\n",
    "Nbprf.add_priors_from_bounds(Nbounds)\n",
    "Nbprf.prep_info()\n",
    "print(Nbprf.init_p_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 542745\n",
    "initial_guess = [\n",
    "    0, # x\n",
    "    0, # y\n",
    "    1, # size\n",
    "    # 1, # amp_1\n",
    "    .1, # amp_2\n",
    "    1, # size_2\n",
    "    1, # b_val\n",
    "    10, # d_val\n",
    "]\n",
    "time_start = datetime.now()\n",
    "with multiprocessing.Pool(n_cpus) as pool:\n",
    "    # We need to set the model for the global model object\n",
    "    # this is because parallel processing does not allow for the model to be pickled\n",
    "    # If it doesn't work the first time, try again... \n",
    "    # for some reason this is not always working the first time...\n",
    "\n",
    "    prfpy_global_model.set_model(dn_model)\n",
    "    Nbprf.run_mcmc_fit(\n",
    "        # initial_guess=initial_guess,\n",
    "        idx=idx, \n",
    "        n_walkers=40,\n",
    "        n_steps=1000, \n",
    "        pool=pool, \n",
    "        eps=1,\n",
    "        # What to save?\n",
    "        # we fit alot of stuff...\n",
    "        # burn_in=500,        # Remove the first n steps as burn in\n",
    "        save_top_kpsc=None,   # Save the top k% of fits? [None for all]\n",
    "        save_min_rsq=None,     # Minimum rsq to save?  [None for all]\n",
    "        save_mode='obj',    # Save mode (obj or minimal). Obj saves the whole object, minimal just the parameters\n",
    "        enforce_bounds=True, # Enforce the bounds\n",
    "    )\n",
    "time_end = datetime.now()\n",
    "print(f\"Time taken: {time_end - time_start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "mprf = Nbprf.sampler[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mprf.pd_params.rsq.argmax())\n",
    "_ = mprf.prf_ts_plot(16)\n",
    "_ = prf.prf_ts_plot(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nbprf.sampler[idx].visual_field(\n",
    "    dot_col='rsq', \n",
    "    th={}, #{'min-step_id':250},\n",
    "    # th={'min-rsq':.1, }, #'min-step_id':500},\n",
    "    do_colbar=True,\n",
    "    # dot_vmin=0, \n",
    "    dot_size=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nbprf.sampler[idx].pd_params.rsq.max()\n",
    "# Nbprf.sampler[idx].prf_params_np[36016,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nbprf.sampler[idx].pd_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vx_mask = np.ones_like(Nbprf.sampler[idx].pd_params.rsq)\n",
    "print(Nbprf.sampler[idx].pd_params.rsq.argmax())\n",
    "vx_mask[Nbprf.sampler[idx].pd_params.rsq < 0.7] = 0\n",
    "# vx_mask[Nbprf.sampler[idx].pd_params.amp_1 < 0] = 0\n",
    "vx_mask[Nbprf.sampler[idx].pd_params.size_1 > Nbprf.sampler[idx].pd_params.size_2] = 0\n",
    "\n",
    "id2see = np.where(vx_mask)[0]\n",
    "old_size2 = -100\n",
    "for i in id2see[:2400]:\n",
    "    size2 = Nbprf.sampler[idx].pd_params.size_2[i]\n",
    "    if np.abs(size2 - old_size2) < 2:\n",
    "        continue\n",
    "    else:\n",
    "        _ = Nbprf.sampler[idx].prf_ts_plot(i)\n",
    "        old_size2 = size2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2see.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nbprf.sampler[idx].prf_ts_plot(11054)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple scatter for several parameters \n",
    "Nbprf.sampler[idx].multi_scatter(\n",
    "    ['size_1', 'size_2','b_val', 'd_val', 'rsq'],\n",
    "    th={'min-rsq':0,}, # 'min-size_ratio':1}, \n",
    "    # th={'min-rsq':0.76, 'min-step_id':500, 'min-b_val':0},\n",
    "    # do_id_line=True, \n",
    "    do_line=True, \n",
    "    dag_scatter=True, \n",
    "    \n",
    ")\n",
    "plt.gcf().set_size_inches(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.percentile(Nbprf.sampler[idx].pd_params.rsq, 85)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prf_fix",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
